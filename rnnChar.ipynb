{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "357168df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "face168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PoetryFoundationData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0938d4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13854"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"Title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e8573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    # Keep letters, space, and select punctuation\n",
    "    text = re.sub(r\"[^a-z\\s.,!?;:'\\\"-]\", '', text)\n",
    "    # Collapse multiple spaces into one\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1c6dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_title'] = df['Title'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "02b3e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new changes\n",
    "titles = df['cleaned_title'].tolist()\n",
    "processed_titles = titles.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "13771bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_text = '\\n'.join(df['cleaned_title'].tolist()) + '\\n'#old\n",
    "all_text = '\\n'.join(processed_titles) + '\\n'#new\n",
    "chars = sorted(set(all_text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2d8d1761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding the text\n",
    "def encode_text(text):\n",
    "    return [char_to_idx[ch] for ch in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cdd09382",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_titles = [encode_text(title) for title in processed_titles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9cd786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "55d85d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data_list):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for seq in data_list:\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        inputs.append(seq[:-1])  # all except last character\n",
    "        targets.append(seq[1:])  # all except first character\n",
    "    return inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c3e1c5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 13823\n"
     ]
    }
   ],
   "source": [
    "X,Y = create_sequences(encoded_titles)\n",
    "print(f\"Number of sequences: {len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1ef108fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, Y, batch_size, pad_token=0):\n",
    "    n = len(X)\n",
    "    indices = list(range(n))\n",
    "    while True:\n",
    "        random.shuffle(indices)\n",
    "        for start in range(0, n, batch_size):\n",
    "            batch_idx = indices[start:start + batch_size]\n",
    "            x_batch = [X[i] for i in batch_idx]\n",
    "            y_batch = [Y[i] for i in batch_idx]\n",
    "\n",
    "            # Find max length in this batch for padding\n",
    "            max_len = max(len(seq) for seq in x_batch)\n",
    "\n",
    "            # Pad sequences with pad_token (typically 0)\n",
    "            x_batch_padded = [seq + [pad_token] * (max_len - len(seq)) for seq in x_batch]\n",
    "            y_batch_padded = [seq + [pad_token] * (max_len - len(seq)) for seq in y_batch]\n",
    "\n",
    "            yield x_batch_padded, y_batch_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "92421b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, vocab_size,hidden_size=512):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "\n",
    "        self.Wxh = np.random.randn(hidden_size, vocab_size) * np.sqrt(2.0/vocab_size)\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * np.sqrt(2.0/hidden_size) \n",
    "        self.Why = np.random.randn(vocab_size, hidden_size) * np.sqrt(2.0/hidden_size)\n",
    "\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "\n",
    "        \n",
    "        self.h_prev = np.zeros((hidden_size, 1))\n",
    "        \n",
    "    def one_hot_encode(self, idx):\n",
    "        \"\"\"Convert index to one-hot encoded vector.\"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        #at each time step, we need four of these dictionaries\n",
    "        xs,hs,ys,ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(self.h_prev)\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = self.one_hot_encode(inputs[t])\n",
    "            hs[t] = np.tanh(np.dot(self.Wxh,xs[t]) + np.dot(self.Whh,hs[t-1])+self.bh)\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by\n",
    "            e_y = np.exp(ys[t] - np.max(ys[t]))\n",
    "            ps[t] = e_y / np.sum(e_y)\n",
    "\n",
    "            \n",
    "\n",
    "        self.last_cache = (xs, hs, ys, ps)\n",
    "        self.h_prev = hs[len(inputs) - 1]\n",
    "\n",
    "        return xs, hs, ys, ps\n",
    "\n",
    "    def sample(self, seed_idx, length, temperature=0.8):\n",
    "    # Add temperature to control randomness\n",
    "        x = self.one_hot_encode(seed_idx)\n",
    "        h = np.copy(self.h_prev)\n",
    "        output = []\n",
    "        for _ in range(length):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            y = y / temperature  # Apply temperature\n",
    "            e_y = np.exp(y - np.max(y))\n",
    "            p = e_y / np.sum(e_y)\n",
    "            idx = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            output.append(idx)\n",
    "            x = self.one_hot_encode(idx)\n",
    "        return output\n",
    "    def lossandgrads(self, targets, pad_token=0):\n",
    "        xs, hs, ys, ps = self.last_cache\n",
    "        loss = 0.0\n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        dh_next = np.zeros_like(hs[0])\n",
    "\n",
    "        valid_timesteps = 0  # count non-padding tokens\n",
    "\n",
    "        for t in reversed(range(len(xs))):\n",
    "            if targets[t] == pad_token:\n",
    "                continue  # skip padding tokens\n",
    "\n",
    "            valid_timesteps += 1\n",
    "            loss += -np.log(ps[t][targets[t], 0] + 1e-9)  # add epsilon to avoid log(0)\n",
    "\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "\n",
    "            # Gradients\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            dh = np.dot(self.Why.T, dy) + dh_next\n",
    "            dh_raw = (1 - hs[t] * hs[t]) * dh\n",
    "            dbh += dh_raw\n",
    "            dWxh += np.dot(dh_raw, xs[t].T)\n",
    "            dWhh += np.dot(dh_raw, hs[t - 1].T)\n",
    "            dh_next = np.dot(self.Whh.T, dh_raw)\n",
    "\n",
    "        # Clip gradients\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -1, 1, out=dparam)\n",
    "\n",
    "        self.grads = (dWxh, dWhh, dWhy, dbh, dby)\n",
    "\n",
    "        # Normalize loss by valid (non-padding) timesteps\n",
    "        return loss / (valid_timesteps if valid_timesteps > 0 else 1)\n",
    "\n",
    "    def update_params(self, learning_rate=1e-3):\n",
    "        dWxh, dWhh, dWhy, dbh, dby = self.grads\n",
    "        for param, dparam in zip([self.Wxh, self.Whh, self.Why, self.bh, self.by],[dWxh, dWhh, dWhy, dbh, dby]):\n",
    "            param -= learning_rate * dparam\n",
    "    \n",
    "    def train_step(self, inputs, targets, learning_rate=1e-3):\n",
    "        \n",
    "        self.forward(inputs)\n",
    "        loss = self.lossandgrads(targets)\n",
    "        self.update_params(learning_rate)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cb3e434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(vocab_size,hidden_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7e6792fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled text starting with 't': ''wen,xy\n",
      "c eczumzw.-q;\"px;rr:.\n"
     ]
    }
   ],
   "source": [
    "seed_char = \"t\"\n",
    "seed_idx = char_to_idx[seed_char]\n",
    "sample_length = 30\n",
    "sampled_indices = model.sample(seed_idx, sample_length)\n",
    "sampled_text = ''.join(idx_to_char[idx] for idx in sampled_indices)\n",
    "print(f\"Sampled text starting with '{seed_char}': {sampled_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "253c1138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.773973\n",
      "Seed 't': tbdra'\n",
      "trv'fbvs\"ts\"' d\"yb katpskitua:.oqdkv,nlw?z!\n",
      "Seed 'a': zbhg:yxttdna\n",
      "r,-;uuyrcrygw!vmghbrde'l-lwr.ro?o qtg\n",
      "Epoch 10, Loss: 3.789886\n",
      "Seed 't': 't-ttem'azkm at;upriimqczdtt'.lbn d ru .  'wh!odve\n",
      "Seed 'a': byzmpttz;ut'nw\n",
      "rtmtmaaazyhvtiuma\"wqfvnr\"x kn\n",
      "!gl!w\n",
      "Epoch 20, Loss: 3.652901\n",
      "Seed 't': g-etgmsdpv';?aroxo .;q?? ej.ltct:iiyavox q.ccj is?\n",
      "Seed 'a': \"bcth.-r eam t\"\"tj \n",
      "jbqkr?ywgwqzlr!a !no!s?o f.!mi\n",
      "Epoch 30, Loss: 3.511940\n",
      "Seed 't': mr\"or j.eie!as  a  v.  e  io ee  ?s ehp  e?s s  ke\n",
      "Seed 'a': zrmzb o,kd r  oe jo'e sie eoe m\n",
      "  b e he          \n",
      "Epoch 40, Loss: 3.394418\n",
      "Seed 't':  m tr  e e e oshs se  a   eeebs          e e   e t\n",
      "Seed 'a':   e  auo e lu a erateshti e sst    ae eanba  mhqoo\n",
      "Epoch 50, Loss: 3.244720\n",
      "Seed 't': ebo iurtiie eern tie,ie oeos e eera seh  seatcs or\n",
      "Seed 'a': beete m bee e o s  n  enltlm mi  e aoet  rse  itcl\n",
      "Epoch 60, Loss: 3.163949\n",
      "Seed 't':  o e  to eioed hro  soeoaree cod on rotie  ir  ie \n",
      "Seed 'a': iiono  inern er  iaer tn r tie  i  noe ioao  e eeo\n",
      "Epoch 70, Loss: 3.254497\n",
      "Seed 't':  te  oi taheaew ta ial  ecopr te ae iiooht  aiis e\n",
      "Seed 'a': ioe tee  o oesair ot ieart to a  oo  esheraso oa s\n",
      "Epoch 80, Loss: 3.181432\n",
      "Seed 't':  uode   oea se sn so  aa eooaoln osstesns  olt en \n",
      "Seed 'a': uon  s oe ti    e e  aese s  netitra hn anduader t\n",
      "Epoch 90, Loss: 3.240810\n",
      "Seed 't': saine sntom sehegseaeensi olhsipesga  loatteysieit\n",
      "Seed 'a':   hroletcrnonate atertaiuitab feirie ertt ti eiaes\n",
      "Epoch 100, Loss: 3.189067\n",
      "Seed 't': teuin inot sttn  sio d e lsle  e eate ssogennetnss\n",
      "Seed 'a': thercihe s tuin  e iodn   son netraaoehs  af eoot \n",
      "Epoch 110, Loss: 2.995389\n",
      "Seed 't': he amnaea ttse  to  ohet iohrae aoo   eto taee e e\n",
      "Seed 'a': ie isr  h ei r  eien eitts  o utra fe of dle  eate\n",
      "Epoch 120, Loss: 3.165100\n",
      "Seed 't': aorlt ooattt aatieli e r e  sno i on erdienl st  e\n",
      "Seed 'a':  thsia y f  oe  to s ora eatneoot e foo  sego tg i\n",
      "Epoch 130, Loss: 3.164564\n",
      "Seed 't':  oh tnhesrrne  iieeirteiet  s ntlh  oorot e e sne \n",
      "Seed 'a': ntus oanersto etieim aeagradeatreef tte voo   oa e\n",
      "Epoch 140, Loss: 3.104562\n",
      "Seed 't': nliaiae eotoe e eitce sant he  e e sy e  o ntftoe \n",
      "Seed 'a':  a f sooi  tnoiimtne e tieioe lnt e ryir e  ene se\n",
      "Epoch 150, Loss: 3.022627\n",
      "Seed 't': onoofopehi  o e  e ioon n  aieees nreis a se a  se\n",
      "Seed 'a': i roo ae en raots hyoe on   ironaee r onit or erio\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x_batch, y_batch):\n\u001b[32m      7\u001b[39m     model.h_prev = np.zeros((model.hidden_size, \u001b[32m1\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     loss = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     total_loss += loss\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:  \u001b[38;5;66;03m# Check more frequently\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[121]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mRNN.train_step\u001b[39m\u001b[34m(self, inputs, targets, learning_rate)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, targets, learning_rate=\u001b[32m1e-3\u001b[39m):\n\u001b[32m    101\u001b[39m     \u001b[38;5;28mself\u001b[39m.forward(inputs)\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlossandgrads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28mself\u001b[39m.update_params(learning_rate)\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[121]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mRNN.lossandgrads\u001b[39m\u001b[34m(self, targets, pad_token)\u001b[39m\n\u001b[32m     80\u001b[39m     dbh += dh_raw\n\u001b[32m     81\u001b[39m     dWxh += np.dot(dh_raw, xs[t].T)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     dWhh += \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdh_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     dh_next = np.dot(\u001b[38;5;28mself\u001b[39m.Whh.T, dh_raw)\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Clip gradients\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "batch_gen = batch_generator(X, Y, 32)\n",
    "for epoch in range(10000):\n",
    "    x_batch, y_batch = next(batch_gen)\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in zip(x_batch, y_batch):\n",
    "        model.h_prev = np.zeros((model.hidden_size, 1))\n",
    "        loss = model.train_step(x, y, learning_rate=1e-5)\n",
    "        total_loss += loss\n",
    "\n",
    "    if epoch % 10 == 0:  # Check more frequently\n",
    "        avg_loss = total_loss / len(x_batch)\n",
    "        print(f\"Epoch {epoch}, Loss: {avg_loss:.6f}\")  # More precision\n",
    "        \n",
    "        # Sample with different seeds\n",
    "        for seed_char in ['<', 't', 'a']:\n",
    "            if seed_char in char_to_idx:\n",
    "                seed_idx = char_to_idx[seed_char]\n",
    "                sample_idxs = model.sample(seed_idx, 50, temperature=0.5)\n",
    "                generated = ''.join(idx_to_char[i] for i in sample_idxs)\n",
    "                print(f\"Seed '{seed_char}': {generated}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f3214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       objects used to prop open a window\n",
       "1                           the new church\n",
       "2                              look for me\n",
       "3                                wild life\n",
       "4                                 umbrella\n",
       "5                                   sunday\n",
       "6                           invisible fish\n",
       "7             dont bother the earth spirit\n",
       "8      the one thing that can save america\n",
       "9     \"hour in which i consider hydrangea\"\n",
       "10                                   stung\n",
       "11                     nothing but good...\n",
       "12                               how quiet\n",
       "13                               porcupine\n",
       "14                           summer apples\n",
       "15               visiting the neighborhood\n",
       "16                                   scars\n",
       "17                        what remains two\n",
       "18                          west of myself\n",
       "19                                     yes\n",
       "Name: cleaned_title, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"cleaned_title\"].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2558d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class ImprovedRNN:\n",
    "    def __init__(self, vocab_size, seq_length, hidden_size=128):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Better weight initialization (Xavier/Glorot)\n",
    "        self.Wxh = np.random.randn(hidden_size, vocab_size) * np.sqrt(2.0 / vocab_size)\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        self.h_prev = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # For Adam optimizer\n",
    "        self.mWxh, self.mWhh, self.mWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        self.mbh, self.mby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        self.vWxh, self.vWhh, self.vWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        self.vbh, self.vby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "\n",
    "    def one_hot_encode(self, idx):\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(self.h_prev)\n",
    "        \n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = self.one_hot_encode(inputs[t])\n",
    "            hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t-1]) + self.bh)\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by\n",
    "            ps[t] = self.softmax(ys[t])\n",
    "        \n",
    "        self.last_cache = (xs, hs, ys, ps)\n",
    "        self.h_prev = hs[len(inputs) - 1]\n",
    "        return xs, hs, ys, ps\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        # Numerical stability\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x)\n",
    "\n",
    "    def sample(self, seed_idx, length, temperature=1.0):\n",
    "        \"\"\"Sample with temperature control\"\"\"\n",
    "        x = self.one_hot_encode(seed_idx)\n",
    "        h = np.copy(self.h_prev)\n",
    "        output = []\n",
    "        \n",
    "        for _ in range(length):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            \n",
    "            # Apply temperature\n",
    "            y = y / temperature\n",
    "            p = self.softmax(y)\n",
    "            \n",
    "            idx = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            output.append(idx)\n",
    "            x = self.one_hot_encode(idx)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def lossandgrads(self, targets):\n",
    "        xs, hs, ys, ps = self.last_cache\n",
    "        loss = 0.0\n",
    "        \n",
    "        dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        dh_next = np.zeros_like(hs[0])\n",
    "        \n",
    "        for t in reversed(range(len(xs))):\n",
    "            loss += -np.log(ps[t][targets[t], 0] + 1e-8)  # Add small epsilon for numerical stability\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "            \n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            \n",
    "            dh = np.dot(self.Why.T, dy) + dh_next\n",
    "            dh_raw = (1 - hs[t] * hs[t]) * dh\n",
    "            dbh += dh_raw\n",
    "            \n",
    "            dWxh += np.dot(dh_raw, xs[t].T)\n",
    "            dWhh += np.dot(dh_raw, hs[t - 1].T)\n",
    "            dh_next = np.dot(self.Whh.T, dh_raw)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        \n",
    "        self.grads = (dWxh, dWhh, dWhy, dbh, dby)\n",
    "        return loss\n",
    "\n",
    "    def update_params_adam(self, learning_rate=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-8, t=1):\n",
    "        \"\"\"Adam optimizer - better than vanilla SGD\"\"\"\n",
    "        dWxh, dWhh, dWhy, dbh, dby = self.grads\n",
    "        \n",
    "        params = [self.Wxh, self.Whh, self.Why, self.bh, self.by]\n",
    "        grads = [dWxh, dWhh, dWhy, dbh, dby]\n",
    "        m_params = [self.mWxh, self.mWhh, self.mWhy, self.mbh, self.mby]\n",
    "        v_params = [self.vWxh, self.vWhh, self.vWhy, self.vbh, self.vby]\n",
    "        \n",
    "        for param, grad, m, v in zip(params, grads, m_params, v_params):\n",
    "            m *= beta1\n",
    "            m += (1 - beta1) * grad\n",
    "            v *= beta2\n",
    "            v += (1 - beta2) * (grad ** 2)\n",
    "            \n",
    "            m_corrected = m / (1 - beta1 ** t)\n",
    "            v_corrected = v / (1 - beta2 ** t)\n",
    "            \n",
    "            param -= learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon)\n",
    "\n",
    "    def train_step(self, inputs, targets, learning_rate=1e-3, t=1):\n",
    "        self.forward(inputs)\n",
    "        loss = self.lossandgrads(targets)\n",
    "        self.update_params_adam(learning_rate, t=t)\n",
    "        return loss\n",
    "\n",
    "# Better training loop with learning rate scheduling and monitoring\n",
    "def train_improved_rnn(model, X, Y, batch_size=32, epochs=100, initial_lr=1e-3):\n",
    "    batch_gen = batch_generator(X, Y, batch_size)\n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        x_batch, y_batch = next(batch_gen)\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Learning rate decay\n",
    "        lr = initial_lr * (0.95 ** (epoch // 10))\n",
    "        \n",
    "        for x, y in zip(x_batch, y_batch):\n",
    "            step += 1\n",
    "            loss = model.train_step(x, y, learning_rate=lr, t=step)\n",
    "            total_loss += loss\n",
    "        \n",
    "        avg_loss = total_loss / len(x_batch)\n",
    "        \n",
    "        # More frequent monitoring\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, LR: {lr:.6f}\")\n",
    "            \n",
    "            # Sample with different temperatures\n",
    "            seed_idx = char_to_idx.get('t', 0)  # Fallback to 0 if 't' not found\n",
    "            \n",
    "            print(\"Temperature 0.5 (conservative):\")\n",
    "            sample_idxs = model.sample(seed_idx, 100, temperature=0.5)\n",
    "            generated = ''.join(idx_to_char[i] for i in sample_idxs)\n",
    "            print(generated[:200])  # First 200 chars\n",
    "            \n",
    "            print(\"\\nTemperature 1.0 (balanced):\")\n",
    "            sample_idxs = model.sample(seed_idx, 100, temperature=1.0)\n",
    "            generated = ''.join(idx_to_char[i] for i in sample_idxs)\n",
    "            print(generated[:200])\n",
    "            \n",
    "            print(\"-\" * 60)\n",
    "\n",
    "# Usage\n",
    "# model = ImprovedRNN(vocab_size=len(char_to_idx), seq_length=100, hidden_size=256)  # Larger hidden size\n",
    "# train_improved_rnn(model, X, Y, batch_size=32, epochs=200, initial_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2c0607c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 422.1057, LR: 0.010000\n",
      "Temperature 0.5 (conservative):\n",
      "q?', tux\";!i :.u\"?'dmb.b;?j: :c\"\"?jf cy.zr\n",
      "oeug.\";is b..\"?!!m:g.q?iombu.q;j! ka.\"?j! :u.\"?zzweu.\"?!'\n",
      "\n",
      "Temperature 1.0 (balanced):\n",
      "a?z:deu??sjjg!-mo?'itlbux-?ombu.;mi?olak\"\n",
      "zotbd\"\";cnmbu.\"qnormqa?z,'ltx,-?j, bu.-q!jptdc;dvom,spa?zj\n",
      "------------------------------------------------------------\n",
      "Epoch 5, Loss: 325.2590, LR: 0.010000\n",
      "Temperature 0.5 (conservative):\n",
      "eeeonshheigirseategeldnato o s\n",
      "\n",
      "tneopstf io mshhsluo \n",
      "ht eeoaettt nir f ig m thhianrssd\n",
      "tre  hhttaor\n",
      "\n",
      "Temperature 1.0 (balanced):\n",
      "'or awu-ir m \n",
      "w\n",
      "niela hy\n",
      "gq  bfxw?eotethanoanjc-ht anilarr e e \n",
      "ygte hoho nopdpjfm be aroonersc wcel\n",
      "------------------------------------------------------------\n",
      "Epoch 10, Loss: 315.2380, LR: 0.009500\n",
      "Temperature 0.5 (conservative):\n",
      "stte\n",
      "agmartti opdt e\n",
      "on sde \n",
      "o bst\n",
      "ere aitre\n",
      "s fptt  egrss e\n",
      "osfet\n",
      "et ens o , efntherogf t irondatte\n",
      "\n",
      "Temperature 1.0 (balanced):\n",
      "ath tyu sehorlonfl\n",
      "lhtcost et orpidolasiit ane pnsgesolf r weosus\n",
      "g\n",
      "e c cudcen wrttk  aslktk\n",
      "gsfpr\n",
      "e\n",
      "------------------------------------------------------------\n",
      "Epoch 15, Loss: 310.5436, LR: 0.009500\n",
      "Temperature 0.5 (conservative):\n",
      "thasseotetasran o aranent frlonantissatetearero tthas\n",
      "ete ssmoe e besorottassa te m sse epar \n",
      "ete me\n",
      "\n",
      "Temperature 1.0 (balanced):\n",
      "? tmah elatsiyponxyononeh,te enon,gocanontsevasoe oma tinehhl\n",
      "t uptnsog omelskocazharirand lsalterus\n",
      "------------------------------------------------------------\n",
      "Epoch 20, Loss: 302.5469, LR: 0.009025\n",
      "Temperature 0.5 (conservative):\n",
      "e i tose le thenre  t\n",
      " are t\n",
      "banrel \n",
      "onan al k\n",
      "er th te bitigangail amanre tin anil hanbatin t oniri\n",
      "\n",
      "Temperature 1.0 (balanced):\n",
      "id y ndapf imovautt:\n",
      "arane k\n",
      "t fa in meerse saneai\n",
      "k\n",
      "soamifcoshhi dlht\n",
      "on oc ,haners\n",
      "a moreh bavode \n",
      "------------------------------------------------------------\n",
      "Epoch 25, Loss: 295.9965, LR: 0.009025\n",
      "Temperature 0.5 (conservative):\n",
      "\n",
      "tobey n te af n t thise te whhat core n te wifn e bavle\n",
      "se whafdte whif e e fnt te wofat owrin t .o\n",
      "\n",
      "Temperature 1.0 (balanced):\n",
      " t opiadt\n",
      "osemef th rafutete ifunainren\n",
      "g wee f\n",
      " de malis \n",
      "ugsy\n",
      "ttagit\n",
      "so j hin  peothekderobehave a\n",
      "------------------------------------------------------------\n",
      "Epoch 30, Loss: 295.2885, LR: 0.008574\n",
      "Temperature 0.5 (conservative):\n",
      "e lr anather soa ur fadeere n de\n",
      "ve bobaper wane ur pafderer these nn base\n",
      "wigomor she ton nr uhe\n",
      "w \n",
      "\n",
      "Temperature 1.0 (balanced):\n",
      "eriw\n",
      "o\n",
      "becle sraderr an\n",
      "hiw  meee e an\n",
      "doveea  polth lasumin\n",
      "lofr\n",
      "y ghtulirsn fho me theufl\n",
      "lac\n",
      "t, s\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m new_model = ImprovedRNN(vocab_size, seq_length, hidden_size=\u001b[32m256\u001b[39m)  \u001b[38;5;66;03m# Larger hidden size\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrain_improved_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 139\u001b[39m, in \u001b[36mtrain_improved_rnn\u001b[39m\u001b[34m(model, X, Y, batch_size, epochs, initial_lr)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x_batch, y_batch):\n\u001b[32m    138\u001b[39m     step += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     loss = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     total_loss += loss\n\u001b[32m    142\u001b[39m avg_loss = total_loss / \u001b[38;5;28mlen\u001b[39m(x_batch)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mImprovedRNN.train_step\u001b[39m\u001b[34m(self, inputs, targets, learning_rate, t)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, targets, learning_rate=\u001b[32m1e-3\u001b[39m, t=\u001b[32m1\u001b[39m):\n\u001b[32m    120\u001b[39m     \u001b[38;5;28mself\u001b[39m.forward(inputs)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlossandgrads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28mself\u001b[39m.update_params_adam(learning_rate, t=t)\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mImprovedRNN.lossandgrads\u001b[39m\u001b[34m(self, targets)\u001b[39m\n\u001b[32m     81\u001b[39m dWhy += np.dot(dy, hs[t].T)\n\u001b[32m     82\u001b[39m dby += dy\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m dh = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mWhy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m)\u001b[49m + dh_next\n\u001b[32m     85\u001b[39m dh_raw = (\u001b[32m1\u001b[39m - hs[t] * hs[t]) * dh\n\u001b[32m     86\u001b[39m dbh += dh_raw\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/numpy/_core/multiarray.py:761\u001b[39m, in \u001b[36mdot\u001b[39m\u001b[34m(a, b, out)\u001b[39m\n\u001b[32m    692\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[33;03m    result_type(*arrays_and_dtypes)\u001b[39;00m\n\u001b[32m    694\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    756\u001b[39m \n\u001b[32m    757\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays_and_dtypes\n\u001b[32m--> \u001b[39m\u001b[32m761\u001b[39m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath.dot)\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdot\u001b[39m(a, b, out=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    763\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    764\u001b[39m \u001b[33;03m    dot(a, b, out=None)\u001b[39;00m\n\u001b[32m    765\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    851\u001b[39m \n\u001b[32m    852\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    853\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b, out)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "new_model = ImprovedRNN(vocab_size, seq_length, hidden_size=256)  # Larger hidden size\n",
    "train_improved_rnn(new_model, X, Y, batch_size=32, epochs=2000, initial_lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "768452c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average title length: 19.56691208315288\n"
     ]
    }
   ],
   "source": [
    "avg_title_length = df['cleaned_title'].str.len().mean()\n",
    "print(f\"Average title length: {avg_title_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e3fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
